# database-optimizer Examples

## Usage Scenarios

This document provides practical examples of using the `database-optimizer` skill in various contexts.

---

## Example 1: PostgreSQL Slow Query Optimization

### Scenario
An e-commerce application has a product search query that takes 4.2 seconds and needs to run under 200ms.

### User Request
```
Please use skill:database-optimizer to optimize our slow product search query on PostgreSQL 15.
```

### Skill Workflow

**Step 1**: Target Identification
- Identifies PostgreSQL 15.4
- User provides the slow query and EXPLAIN ANALYZE output
- Target: reduce from 4200ms to under 200ms
- Query runs ~500 times/minute during peak hours

**Step 2**: Socratic Questions
```
I've reviewed the query and execution plan. Before proceeding:

1. Is the products table partitioned, and how many rows does it contain?
2. Are there any covering indexes already on products?
3. Is this query generated by an ORM or hand-written SQL?
4. Do you have pg_stat_statements enabled for workload context?
5. Is online (CONCURRENTLY) index creation acceptable in production?
```

**User Response**:
```
1. Not partitioned, 12M rows
2. Only primary key and a B-tree on sku
3. Generated by SQLAlchemy, but we can add raw SQL
4. Yes, pg_stat_statements is active
5. Yes, CONCURRENTLY is fine
```

**Step 3**: Performance Data Collection
```sql
-- Original query
EXPLAIN (ANALYZE, BUFFERS, FORMAT TEXT)
SELECT p.id, p.name, p.price, p.sku, c.name AS category
FROM products p
JOIN categories c ON c.id = p.category_id
WHERE p.name ILIKE '%wireless headphone%'
  AND p.price BETWEEN 50 AND 200
  AND p.is_active = true
ORDER BY p.rating DESC
LIMIT 20;

-- Execution plan (problematic)
Limit (cost=158432.10..158432.15 rows=20 width=86) (actual time=4187.321..4187.340 rows=20 loops=1)
  -> Sort (cost=158432.10..158478.22 rows=18450 width=86) (actual time=4187.318..4187.330 rows=20 loops=1)
        Sort Key: p.rating DESC
        Sort Method: top-N heapsort  Memory: 28kB
        -> Hash Join (cost=12.50..157891.44 rows=18450 width=86) (actual time=0.412..4152.108 rows=18622 loops=1)
              -> Seq Scan on products p (cost=0.00..157722.00 rows=18450 width=78) (actual time=0.088..4138.221 rows=18622 loops=1)
                    Filter: (is_active AND (price >= 50) AND (price <= 200) AND (name ~~* '%wireless headphone%'))
                    Rows Removed by Filter: 11981378
              -> Hash (cost=8.00..8.00 rows=360 width=12) (actual time=0.291..0.292 rows=360 loops=1)
Planning Time: 0.182 ms
Execution Time: 4187.401 ms
Buffers: shared hit=42108 read=89204

-- Index usage stats
SELECT indexrelname, idx_scan, idx_tup_read
FROM pg_stat_user_indexes
WHERE relname = 'products';
```

**Step 4**: Analysis
- **Root cause**: Sequential scan on 12M-row products table (no usable index for ILIKE + range filter)
- **Bottleneck**: 11.9M rows filtered out after full table read
- **I/O**: 89,204 buffer reads from disk — table doesn't fit in cache
- **Opportunity**: Trigram GIN index for ILIKE pattern matching, partial index for is_active

**Step 5**: Optimization Report

```markdown
# Query Optimization Report

## Executive Summary
- **Query**: Product search with text pattern, price range, and active filter
- **Current Time**: 4,187ms
- **Target Time**: <200ms
- **Expected After Optimization**: ~35ms (99.2% improvement)

## Root Cause Analysis
The query performs a sequential scan on 12M rows because:
1. ILIKE '%pattern%' cannot use standard B-tree indexes
2. No index covers the price range + is_active filter combination
3. Sort on rating requires reading all matching rows before LIMIT

## Recommendations

### 1. Trigram GIN Index for Text Search (HIGH IMPACT)
```sql
CREATE EXTENSION IF NOT EXISTS pg_trgm;

CREATE INDEX CONCURRENTLY idx_products_name_trgm
ON products USING GIN (name gin_trgm_ops)
WHERE is_active = true;
```
- **Impact**: Eliminates sequential scan for ILIKE patterns
- **Space**: ~180MB estimated
- **Creation time**: ~3 minutes (non-blocking with CONCURRENTLY)

### 2. Composite B-tree for Price + Rating (MEDIUM IMPACT)
```sql
CREATE INDEX CONCURRENTLY idx_products_price_rating
ON products (price, rating DESC)
WHERE is_active = true;
```
- **Impact**: Enables index scan for price range, pre-sorted by rating
- **Space**: ~95MB estimated

### 3. Optimized Query Rewrite
```sql
-- Original
SELECT p.id, p.name, p.price, p.sku, c.name AS category
FROM products p
JOIN categories c ON c.id = p.category_id
WHERE p.name ILIKE '%wireless headphone%'
  AND p.price BETWEEN 50 AND 200
  AND p.is_active = true
ORDER BY p.rating DESC
LIMIT 20;

-- Optimized: Use trigram similarity with explicit index hint ordering
SELECT p.id, p.name, p.price, p.sku, c.name AS category
FROM products p
JOIN categories c ON c.id = p.category_id
WHERE p.name ILIKE '%wireless headphone%'
  AND p.price BETWEEN 50 AND 200
  AND p.is_active = true
ORDER BY p.rating DESC
LIMIT 20;
```
- Query text stays the same; the GIN index changes the plan automatically
- Alternatively, for even faster full-text search:
```sql
-- Full-text search variant (faster for word-based matching)
SELECT p.id, p.name, p.price, p.sku, c.name AS category
FROM products p
JOIN categories c ON c.id = p.category_id
WHERE p.search_vector @@ plainto_tsquery('english', 'wireless headphone')
  AND p.price BETWEEN 50 AND 200
  AND p.is_active = true
ORDER BY p.rating DESC
LIMIT 20;
```

### Expected Execution Plan After Optimization
```
Limit (cost=284.50..284.55 rows=20 width=86) (actual time=32.112..32.128 rows=20 loops=1)
  -> Sort (cost=284.50..330.62 rows=18450 width=86) (actual time=32.109..32.120 rows=20 loops=1)
        Sort Key: p.rating DESC
        Sort Method: top-N heapsort  Memory: 28kB
        -> Nested Loop (cost=88.42..240.18 rows=18450 width=86) (actual time=1.204..28.887 rows=18622 loops=1)
              -> Bitmap Heap Scan on products p (cost=88.00..195.30 rows=18450 width=78) (actual time=1.148..12.443 rows=18622 loops=1)
                    Recheck Cond: (name ~~* '%wireless headphone%')
                    Filter: (price >= 50 AND price <= 200)
                    -> Bitmap Index Scan on idx_products_name_trgm (cost=0.00..83.39 rows=18500 width=0) (actual time=0.892..0.893 rows=18890 loops=1)
              -> Index Scan using categories_pkey on categories c (cost=0.42..0.44 rows=1 width=12)
Planning Time: 0.298 ms
Execution Time: 32.201 ms
Buffers: shared hit=1842

## Risk Assessment
| Change | Write Impact | Lock Risk | Rollback |
|--------|-------------|-----------|----------|
| pg_trgm GIN index | +2% write overhead | None (CONCURRENTLY) | DROP INDEX |
| Composite B-tree | +1% write overhead | None (CONCURRENTLY) | DROP INDEX |
| Statistics refresh | None | None | N/A |

## Implementation Plan
1. Enable pg_trgm extension (requires superuser, instant)
2. Create GIN index CONCURRENTLY during off-peak (3 min)
3. Create composite index CONCURRENTLY (1 min)
4. Run ANALYZE products to refresh statistics
5. Verify new plan with EXPLAIN ANALYZE
6. Monitor pg_stat_statements for 24 hours
```

---

## Example 2: MySQL N+1 Query Elimination

### Scenario
A REST API endpoint loading orders with items takes 1.8 seconds due to N+1 query pattern detected in slow query log.

### User Request
```
Our /api/orders endpoint is slow. The MySQL slow query log shows hundreds of small queries per request. Help optimize.
```

### Skill Workflow

**Step 1-2**: MySQL 8.0 identification, context loading, memory check

**Step 3**: Performance Data Collection
```sql
-- From slow query log analysis: pattern detected
-- Query 1 (executed once):
SELECT * FROM orders WHERE user_id = 42 ORDER BY created_at DESC LIMIT 20;
-- Query 2 (executed 20 times, once per order):
SELECT * FROM order_items WHERE order_id = ?;
-- Query 3 (executed ~60 times, once per item):
SELECT name, price FROM products WHERE id = ?;

EXPLAIN FORMAT=JSON
SELECT * FROM order_items WHERE order_id = 1001;
-- Shows: type: ALL (full table scan — no index on order_id!)

SHOW INDEX FROM order_items;
-- Only PRIMARY KEY on id, no index on order_id
```

**Step 4**: Analysis
- **N+1 pattern**: 1 + 20 + ~60 = ~81 queries per request
- **Missing index**: order_items.order_id has no index (full scan per lookup)
- **No JOIN**: ORM generating individual SELECT statements instead of JOIN

**Step 5**: Optimization Report

```markdown
# N+1 Query Optimization Report

## Executive Summary
- **Problem**: 81 individual queries per API request (N+1 pattern)
- **Current Time**: 1,800ms (81 queries × ~22ms avg)
- **Target**: Single query, <50ms
- **Root Causes**: N+1 ORM pattern + missing index on order_items.order_id

## Solution

### 1. Add Missing Index (CRITICAL)
```sql
CREATE INDEX idx_order_items_order_id ON order_items(order_id);
```
- Individual order_items lookups drop from 22ms to 0.3ms
- Even without fixing N+1, total drops to ~25ms

### 2. Consolidated Query (RECOMMENDED)
```sql
-- Replace 81 queries with one:
SELECT
    o.id AS order_id,
    o.status,
    o.created_at,
    o.total_amount,
    oi.id AS item_id,
    oi.quantity,
    oi.unit_price,
    p.name AS product_name,
    p.price AS product_price
FROM orders o
JOIN order_items oi ON oi.order_id = o.id
JOIN products p ON p.id = oi.product_id
WHERE o.user_id = 42
ORDER BY o.created_at DESC
LIMIT 20;
```

### 3. Covering Index for Orders Lookup
```sql
CREATE INDEX idx_orders_user_created
ON orders(user_id, created_at DESC);
```

### Expected Results
| Metric | Before | After |
|--------|--------|-------|
| Queries per request | 81 | 1 |
| Total execution time | 1,800ms | 12ms |
| Buffer reads | ~4,200 | ~180 |

## ORM Fix (SQLAlchemy Example)
```python
# Before: N+1 (lazy loading)
orders = session.query(Order).filter_by(user_id=42).limit(20).all()
for order in orders:
    for item in order.items:  # N+1 here
        print(item.product.name)  # N+1 again

# After: Eager loading with joinedload
from sqlalchemy.orm import joinedload

orders = (
    session.query(Order)
    .options(joinedload(Order.items).joinedload(OrderItem.product))
    .filter_by(user_id=42)
    .order_by(Order.created_at.desc())
    .limit(20)
    .all()
)
```

## Implementation Plan
1. Add index on order_items.order_id (immediate, ~30s on 500K rows)
2. Add covering index on orders(user_id, created_at) (immediate)
3. Update ORM query to use joinedload (code change)
4. Deploy and verify with EXPLAIN
5. Monitor slow query log for 48 hours
```

---

## Example 3: SQL Server Query Store Analysis

### Scenario
A reporting dashboard has regressed after a SQL Server statistics update. Queries that ran in 200ms now take 8 seconds.

### User Request
```
After last night's maintenance window, our reporting queries regressed badly on SQL Server 2019. Can you analyze Query Store data and fix the plan regression?
```

### Skill Workflow

**Step 1-2**: SQL Server 2019 identification, context loading

**Step 3**: Performance Data Collection
```sql
-- Identify regressed queries from Query Store
SELECT
    q.query_id,
    qt.query_sql_text,
    rs.avg_duration / 1000.0 AS avg_duration_ms,
    rs.last_duration / 1000.0 AS last_duration_ms,
    rs.count_executions,
    p.plan_id,
    p.is_forced_plan
FROM sys.query_store_query q
JOIN sys.query_store_query_text qt ON q.query_text_id = qt.query_text_id
JOIN sys.query_store_plan p ON q.query_id = p.query_id
JOIN sys.query_store_runtime_stats rs ON p.plan_id = rs.plan_id
WHERE rs.avg_duration / 1000.0 > 5000
ORDER BY rs.avg_duration DESC;

-- Check for missing index recommendations
SELECT
    d.statement AS query,
    d.equality_columns,
    d.inequality_columns,
    d.included_columns,
    gs.avg_user_impact
FROM sys.dm_db_missing_index_details d
JOIN sys.dm_db_missing_index_groups g ON d.index_handle = g.index_handle
JOIN sys.dm_db_missing_index_group_stats gs ON g.index_group_handle = gs.group_handle
WHERE d.database_id = DB_ID()
ORDER BY gs.avg_user_impact DESC;
```

**Step 4**: Analysis
- **Plan regression**: Query Store shows plan_id 47 (good, hash join) replaced by plan_id 52 (bad, nested loop)
- **Cause**: Statistics update changed cardinality estimates on the sales_fact table
- **Row estimate**: Planner estimated 150 rows, actual was 1.2M rows

**Step 5**: Optimization Report

```markdown
# Plan Regression Analysis Report

## Executive Summary
- **Root Cause**: Statistics update caused cardinality misestimate (150 estimated vs 1.2M actual)
- **Impact**: Planner switched from hash join (200ms) to nested loop (8,200ms)
- **Fix**: Force the known-good plan via Query Store + update statistics with fullscan

## Immediate Fix: Force Good Plan
```sql
-- Force the previously good plan (plan_id 47)
EXEC sp_query_store_force_plan @query_id = 1284, @plan_id = 47;
```
- **Impact**: Immediate, no restart required
- **Risk**: Low — plan 47 was stable for 6 months

## Permanent Fix: Improve Statistics
```sql
-- Update statistics with full scan for accurate cardinality
UPDATE STATISTICS dbo.sales_fact WITH FULLSCAN;
UPDATE STATISTICS dbo.dim_product WITH FULLSCAN;

-- Add multi-column statistics for correlated filters
CREATE STATISTICS stat_sales_region_date
ON dbo.sales_fact (region_id, sale_date)
WITH FULLSCAN;
```

## Prevention: Query Store Hints
```sql
-- Add query hint to prevent nested loop on this query pattern
EXEC sp_query_store_set_hints
    @query_id = 1284,
    @query_hints = N'OPTION (HASH JOIN)';
```

## Monitoring
```sql
-- Alert on plan changes for critical queries
SELECT query_id, plan_id, last_compile_start_time
FROM sys.query_store_plan
WHERE query_id IN (1284, 1291, 1305)
ORDER BY last_compile_start_time DESC;
```
```

---

## Example 4: Index Consolidation and Cleanup

### Scenario
A PostgreSQL database has accumulated 340 indexes over 5 years. Many are unused or redundant, costing write performance and disk space.

### User Request
```
We have too many indexes on our PostgreSQL database. Help identify which ones to drop safely.
```

### Skill Workflow

**Step 2**: Clarifying Questions
```
For index cleanup analysis:

1. Can you run pg_stat_user_indexes to get usage stats?
2. How long has the database been running since last stats reset?
3. Are there batch jobs that run weekly/monthly using different indexes?
4. What is the total index size vs table data size?
```

**Step 3**: Data Collection
```sql
-- Unused indexes (zero scans since stats reset)
SELECT schemaname, relname, indexrelname, idx_scan,
       pg_size_pretty(pg_relation_size(indexrelid)) AS index_size
FROM pg_stat_user_indexes
WHERE idx_scan = 0
  AND indexrelname NOT LIKE '%_pkey'
  AND indexrelname NOT LIKE '%_unique%'
ORDER BY pg_relation_size(indexrelid) DESC;

-- Duplicate/overlapping indexes
SELECT a.indexrelid::regclass AS index_a,
       b.indexrelid::regclass AS index_b,
       pg_size_pretty(pg_relation_size(a.indexrelid)) AS size_a
FROM pg_index a
JOIN pg_index b ON a.indrelid = b.indrelid
  AND a.indexrelid != b.indexrelid
  AND a.indkey::text LIKE b.indkey::text || '%'
WHERE a.indisunique = false;
```

**Step 5**: Cleanup Report

```markdown
# Index Cleanup Report

## Summary
- **Total indexes**: 340
- **Unused indexes**: 47 (0 scans in 90 days)
- **Duplicate indexes**: 12 (subsets of other indexes)
- **Space recoverable**: 8.2 GB
- **Write performance gain**: ~15% estimated

## Safe to Drop (47 Unused Indexes)

### High Confidence (no scans, no unique constraint)
| Index | Table | Size | Last Scan | Action |
|-------|-------|------|-----------|--------|
| idx_orders_legacy_status | orders | 890 MB | Never | DROP |
| idx_users_old_email | users | 340 MB | Never | DROP |
| idx_products_temp_sort | products | 280 MB | Never | DROP |
[... 44 more ...]

### Drop Script
```sql
-- Phase 1: Drop unused indexes (run during off-peak)
DROP INDEX CONCURRENTLY idx_orders_legacy_status;
DROP INDEX CONCURRENTLY idx_users_old_email;
DROP INDEX CONCURRENTLY idx_products_temp_sort;
-- [...]

-- Phase 2: Drop duplicates
DROP INDEX CONCURRENTLY idx_orders_user_id; -- subset of idx_orders_user_id_created
DROP INDEX CONCURRENTLY idx_products_cat;   -- subset of idx_products_cat_price
-- [...]
```

## Rollback Script
```sql
-- Recreate if any query regresses (keep for 30 days)
CREATE INDEX CONCURRENTLY idx_orders_legacy_status ON orders(legacy_status);
-- [...]
```

## Monitoring After Cleanup
```sql
-- Check for sequential scans on previously indexed columns
SELECT relname, seq_scan, seq_tup_read
FROM pg_stat_user_tables
WHERE seq_scan > 100
ORDER BY seq_tup_read DESC;
```
```

---

## Common Patterns

### Pattern: Slow Query Fix
```
Optimize this slow query: [paste SQL and EXPLAIN output]
```

### Pattern: Index Audit
```
Analyze all indexes on our database and recommend additions and removals
```

### Pattern: Plan Regression Investigation
```
Query performance degraded after [event]. Analyze execution plans and fix the regression.
```

### Pattern: ORM Query Optimization
```
Our Django/SQLAlchemy/Entity Framework queries are slow. Analyze the generated SQL and suggest optimizations.
```

### Pattern: Workload Tuning
```
Analyze pg_stat_statements / Performance Schema output and prioritize the top optimization opportunities.
```

---

**Last Updated**: 2025-02-06
**Maintained by**: The Forge
